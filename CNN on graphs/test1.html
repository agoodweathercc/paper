<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<ul>
<li>Spectral Networks and deep locally connected networks on graphs</li>
</ul>
<p>CNN can exploit the following structure:</p>
<ol style="list-style-type: decimal">
<li><p>translation structure, allowing the use of filters instead of generic linear maps and hence weight sharing</p></li>
<li><p>metric on the grid, allowing compactly supported filters, whose support is typically much smaller than the size of the input signals</p></li>
<li><p>multiscale dyadic clustering of grid, allowing subsampling, implemented through stride convolutions and pooling</p></li>
</ol>
<p><strong>computational complexity of CNN: n input coordinates(?) </strong></p>
<p>on a d-dim grid, a fully connected layer with m outputs requires n*m parameters.(independent of dimension?)</p>
<p>using small filter reduces complexity to O(n) per feature map(?)</p>
<p>using multiscale dyadic clustering allows each succesive layer to use a factor of 2^d less coordinates per filter(?)</p>
<p><strong>Motivation of studying cnn on graphs</strong></p>
<p>data defined on 3-D meshes, such as surface tension or temperature, measurements from a network of meteorological station, or data coming from social networks or collabolative filtering</p>
<p>another example: intermediate representation arising from deep neural networks(?)</p>
<p><strong>spatial convolutional structure can be exploited at seveal layers, typicall CNN don't assume any geometry in the feature dimension, resulting in 4-D tensor which are only convolutional along their spatial coordinates</strong></p>
<p>two different constructions: first one(spatial construction) extend properties (2) and (3) to general graphs(so no weight sharing? Yes. No esay way to induce weight sharing across different locations of the graph), O(n) parameters instead of O(n^2)</p>
<p>second one(spectral constractions) draw on properties on covolutions in Fourier domain. In R^d, convolutions are linear operators diagonalised by the Fourier basis exp(i.w.t). we extend convolutions to general graphs by finding corresponding Fourier basis(Graph Laplacian arises). O(n) parapmeters per feature map.</p>
<p><strong>Smoothness and Sparsity</strong></p>
<p>filters are multipliers on the eigenvalues of the laplacian <span class="math">Δ</span></p>
<p>functions that are smooth relative to the grid metric have coefficient with quick decay in the basis of eigenvectors of laplacian</p>
<p>The eigenvectors of the subsampled Laplacian are the low frequency eigenvecots of laplacian(why?) <br /><span class="math"><em>x</em><sub><em>k</em> + 1, <em>j</em></sub> = <em>h</em>(<em>V</em>Σ<sub><em>i</em> = 1</sub><sup><em>f</em><sub><em>k</em> − 1</sub></sup><em>F</em><sub><em>k</em>, <em>i</em>, <em>j</em></sub><em>V</em><sup><em>T</em></sup><em>x</em><sub><em>k</em>, <em>j</em></sub>)</span><br /> Cons: most graphs have meaningful eigenvectors only for the very top of the spectrum; not obvious how to do either the forwardprop or backdrop efficiently while applying the nonlinearity on space side.</p>
<p>​</p>
<p>​</p>
<p>​</p>
<p>​</p>
</body>
</html>
